{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to NLP - Pyladies_SaskiaLensink.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santwijck/Jupyter-refactoring-beginner-jul2020/blob/master/Introduction_to_NLP_Pyladies_ellesvanderlaan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pahbT6DVkxNI"
      },
      "source": [
        "# Introduction to NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWwpsULCk0_g"
      },
      "source": [
        "This notebook will introduce you to several traditional NLP techniques, starting with part-of-speech tagging, dependency parsing, and named entity recognition. We will briefly explore ways to translate text to digits so that text can be used in machine learning models, and will go over some supervised machine learning methods known to be effective for text data.  \n",
        "\n",
        "First, import the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrhMdbFZbiq5"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import re \n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# initialize pandas progress bar\n",
        "tqdm.pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nokACOGmuq4"
      },
      "source": [
        "## Analyze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfkS3W1rD-RF"
      },
      "source": [
        "### Split into sentences\n",
        "\n",
        "In order to work with text, it is often necessary to split a text into its separate sentences, and each sentence into its separate words. I often use the `nltk` library (Natural Language Toolkit) to do that for me. Before showing how to use `nltk` to do that, let's first explore why it's easier to use `nltk` than to write your own functions.\n",
        "\n",
        "***TASK | 5 min ***\n",
        "Write some code that splits a text into a list of its sentences, and test it on the paragraphs below. If you're done early, try to improve your paragraph-splitter and test it on paragraph_3. \n",
        "\n",
        "***Hint*** \n",
        "All strings in Python have a `.split()` method that allows you to split a string into smaller strings based on a splitter or separator. Use this `.split()` to split up the following paragraph.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVC2AFdpG4gs"
      },
      "source": [
        "paragraph_1 = ''' In the year of 1878 I took my degree of Doctor of Medicine of \n",
        "the University of London, and proceeded to Netley to go through the course \n",
        "prescribed for surgeons in the Army. Having completed my studies there, I was \n",
        "duly attached to the Fifth Northumberland Fusiliers as assistant surgeon. The \n",
        "regiment was stationed in India at the time, and before I could join it, the \n",
        "second Afghan war had broken out. On landing at Bombay, I learned that my corps \n",
        "had advanced through the passes, and was already deep in the enemy's country. \n",
        "I followed, however, with many other officers who were in the same situation as \n",
        "myself, and succeeded in reaching Candahar in safety, where I found my regiment, \n",
        "and at once entered upon my new duties. '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7fLDKl4KkSK"
      },
      "source": [
        "sentences = paragraph_1.###your code here###\n",
        "sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLBDuOZiLWu3"
      },
      "source": [
        "Now try the same code on the next paragraph. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThNjPPcuHKcR"
      },
      "source": [
        "paragraph_2 = '''I had called upon my friend, Mr. Sherlock Holmes, one day in the \n",
        "autumn of last year and found him in deep conversation with a very stout, \n",
        "florid-faced, elderly gentleman with fiery red hair. With an apology for my \n",
        "intrusion, I was about to withdraw when Holmes pulled me abruptly into the room \n",
        "and closed the door behind me.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NLMFrIDL7BA"
      },
      "source": [
        "sentences = paragraph_2.###your code here###\n",
        "sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L02DLCUfNa9P"
      },
      "source": [
        "paragraph_3 = '''Isa Whitney, the brother of the late Elias Whitney, D.D., \n",
        "Principal of the Theological College of St. George's, was much addicted to \n",
        "opium. The habit grew upon him, as I understand, from some foolish freak when \n",
        "he was at college; for having read De Quincy's descriptions of his dreams and \n",
        "sensations, he had drenched his tobacco with laudanum in any attempt to produce \n",
        "the same effects. He found, as so many more have done, that the practice is \n",
        "easier to attain than to get rid of, and for many years he continued to be a \n",
        "slave to the drug, an object of mingled horror and pity to his friends and \n",
        "relatives.  '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki6MB9a_OVDF"
      },
      "source": [
        "### your code here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d9d91uJO6OT"
      },
      "source": [
        "As you've seen, it is not so trivial to split a paragraph into sentences - at least some language-specific knowledge is needed to understand that the period in 'Mr. Sherlock Holmes' is not the end of a sentence, but part of the abbreviation of Mister. \n",
        "\n",
        "If the 5 minutes haven't passed yet, offer your help to your fellow Pyladies in the chat.\n",
        "\n",
        "===========================================\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb6wgxi3JlTz"
      },
      "source": [
        "`NLTK` is able to take these language-specific uses of periods etc. into account, and does a great job at splitting the above paragraphs into sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTAxRoadPoTt"
      },
      "source": [
        "nltk.sent_tokenize(paragraph_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOJG7sCPProF"
      },
      "source": [
        "nltk.sent_tokenize(paragraph_2)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwixuqYwOa_N"
      },
      "source": [
        "nltk.sent_tokenize(paragraph_3)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CiQZniyP6iz"
      },
      "source": [
        "### Split into words\n",
        "\n",
        "As with sentences, it is also not so trivial to split a texts into its separate words by just using a space character as a splitter. Consider for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OijiYeBuQICL"
      },
      "source": [
        "sentence = \"It's great that you're joining this workshop!\"\n",
        "words = sentence.split(' ')\n",
        "words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59YQvp6-QnpV"
      },
      "source": [
        "Again, `nltk` is of great help:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjga0SctQ5ap"
      },
      "source": [
        "words = nltk.word_tokenize(sentence)\n",
        "words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYBQNDiB9SuK"
      },
      "source": [
        "### Tag\n",
        "Sometimes it is useful to only extract all nouns from a text, or all adjectives plus nouns, or all verbs that combine with the noun 'NLP'. In order to do so, we need to *tag* our text, and assign *part-of-speech (POS)* tags to every word. \n",
        "\n",
        "`nltk` has a build-in tagger that can do this for you. Don't forget that the tagger needs a list of words as input!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4z98dc4-gF4"
      },
      "source": [
        "sentence = \"It's great that you're joining this workshop!\"\n",
        "words = nltk.word_tokenize(sentence)\n",
        "nltk.pos_tag(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVkg9-2dEA4F"
      },
      "source": [
        "# If you are interested in learning what all the abbreviations stand for: \n",
        "nltk.help.upenn_tagset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGCv7qZvD37G"
      },
      "source": [
        "Even though `nltk`'s taggers are working well, I often default to the `spaCy` library to do POS tagging for me. This is mostly because `spaCy` is really easy to use, and provides you with a whole range of different NLP functionalities.\n",
        "\n",
        "In just a few lines of code, `spaCy` will tokenize and POS tag a text for you, and a whole bunch of other things, such as perform dependency parsing, lemmatize, and perform Named Entity Recognition - more on that below. The only steps you need to take are:\n",
        "1. Read in a (statistical) language model that can process your text;\n",
        "2. Use this model to create a `Doc` object;\n",
        "3. Take out the information you need from the `Doc` object.\n",
        "\n",
        "As noted on the spacy.io website:\n",
        "Even though a `Doc` is processed – e.g. split into individual words and annotated – it still holds all information of the original text, like whitespace characters. You can always get the offset of a token into the original string, or reconstruct the original by joining the tokens and their trailing whitespace. This way, you’ll never lose any information when processing text with `spaCy`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgBDOg-WFbl_"
      },
      "source": [
        "# load spacy model (usually called 'nlp')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# create a Doc object\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# inspect contents of doc\n",
        "[(token.text, token.pos_, token.lemma_, token.dep_) for token in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJswxl0hKmnJ"
      },
      "source": [
        "For more information on what kinds of information you can access in a `Doc` object, check https://spacy.io/api/doc and https://spacy.io/api/token. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C4tsydR9idV"
      },
      "source": [
        "### Parse\n",
        "\n",
        "To further explore a text's grammatical structure, you might want to know which words are the subjects of a sentence, which adjectives belong to which nouns, etc. For this, you can use a dependency parser. \n",
        "\n",
        "In the code snippet above, we already saw some examples of this dependency parser: the `.dep_` attributes showed us which words are used as subjects and which words are used as (direct) objects in the example sentence. However, it would help to actually see how the words are related to each other. Luckily, there's `displaCy`  - a visualizer of syntactic dependencies. Try it out for yourself: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KzhKvNeMWj5"
      },
      "source": [
        "# text = 'some text here'\n",
        "# doc = nlp(text)\n",
        "spacy.displacy.render(doc, style='dep', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGJoR5vWOKf5"
      },
      "source": [
        "### Named Entities\n",
        "\n",
        "Last but not least, this notebook will demonstrate how to use `spaCy` to extract named entities from your text, and render a beautiful visualization of them.\n",
        "\n",
        "Named entities are real-world objects such as persons, locations, organizations, etc. They are often quite important in text mining applications that e.g. want to explore what persons or locations are mentioned in a text. Again, `spaCy` can help us detecting them with just a few simple lines of code. Check https://spacy.io/api/annotation if you want to know what all the abbreviations stand for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi9SROJLQc1G"
      },
      "source": [
        "# Remember paragraph 1?\n",
        "print(paragraph_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M1rJet9O38m"
      },
      "source": [
        "doc = nlp(paragraph_1)\n",
        "[(ent.text, ent.label_) for ent in doc.ents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jihbnbuQ16Y"
      },
      "source": [
        "And again, `displaCy` allows you to visualize the different entities in a running text very easily:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_WoV5qrRBZN"
      },
      "source": [
        "spacy.displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpufaCxGMsBo"
      },
      "source": [
        "You can nowadays also relatively easy use state-of-the-art transformer models, by using pipelines from the transformers library. The syntax is straighforward, and you can easily load a different tokenizer and language model. For more information on how to use transformers' pipelines, see https://huggingface.co/transformers/main_classes/pipelines.html. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kzgB0YcMTP6"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trpTrzQ0LzoE"
      },
      "source": [
        "from transformers import pipeline\n",
        "nlp = pipeline(\"ner\")\n",
        "sequence = paragraph_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVivQIy1MjzY"
      },
      "source": [
        "print(nlp(sequence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0TL7y4-O7zw"
      },
      "source": [
        "Note how the text has been tokenized - what stands out?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az58CTu0m6Px"
      },
      "source": [
        "## Prepare\n",
        "\n",
        "The techniques shown above are all very useful when you want to understand the linguistic structure of your text better, and/or when you need to extract specific patterns or entities from a text. However, when you want to fit machine learning models on text, there many other things you can do with text which are collectively known as *text preprocessing*. In text preprocessing you explore, normalize, and vectorize text. Below, you will see what is meant by these three terms. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8yB2Ed0k7dx"
      },
      "source": [
        "### Load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ9MKNdVb1OX"
      },
      "source": [
        "We will work with the 20 newsgroup text dataset. It contains about 18,000 newsgroup posts on 20 different topics. This dataset is often used in NLP workshops, trainings, and tests - you may have encountered it before.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9gH7HTscxgZ"
      },
      "source": [
        "# Reading in the 20 newsgroups dataset as a pandas dataframe:\n",
        "dataset = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTPpiMxclpPf"
      },
      "source": [
        "# Check which categories are part of the data:\n",
        "list(dataset.target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLtsL6EXmTM6"
      },
      "source": [
        "# For now, we'll only select a subset of the labels. \n",
        "labels = ['comp.graphics', 'rec.autos', 'sci.space']\n",
        "\n",
        "# read in a subset of the data:\n",
        "dataset = fetch_20newsgroups(categories=labels, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# change into a pandas dataframe\n",
        "df = pd.DataFrame({\n",
        "    'label':dataset.target,\n",
        "    'text':dataset.data\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgwVF3yflAYQ"
      },
      "source": [
        "### Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znuWG2BWUFfz"
      },
      "source": [
        "#### Explore\n",
        "\n",
        "Start exploring the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IhTZ2KZUNwa"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKJjTazSYdsp"
      },
      "source": [
        "print(df.shape)\n",
        "print(df['label'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n90Xn6SuUWdu"
      },
      "source": [
        "***TASK | 5 min ***\n",
        "Now start exploring the text data yourself. Some questions to consider when exploring text data:\n",
        "1. What does a single item of text look like? Does it consist of multiple words, multiple sentences?\n",
        "2. Are there any special characters in the text that we need to remove or replace? \n",
        "3. Do the texts contain 'general' language, or is there a lot of domain-specific jargon used?\n",
        "4. Are all texts looking the same?\n",
        "5. Are all texts in the same language?\n",
        "... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw41GumXUNTj"
      },
      "source": [
        "# Your own code here (5 min)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze1BBB0PZcTj"
      },
      "source": [
        "If you're done early, offer your help in the chat to your fellow Pyladies!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZBfDCNzZiNi"
      },
      "source": [
        "=================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSNqLNZsZkZN"
      },
      "source": [
        "Another way to explore your text is by clustering it based on the distribution of the words in the different texts. *Topic modeling* is a technique that allows you to explore different clusters (or 'topics') that are present in your text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl_g-1AiZzpD"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t8JITzFapzp"
      },
      "source": [
        "import gensim\n",
        "from gensim import corpora, models\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "\n",
        "tokenized_data = df['text'].apply(nltk.word_tokenize)\n",
        "# Build a Dictionary - association word to numeric id\n",
        "dictionary = corpora.Dictionary(tokenized_data)\n",
        "# Transform the collection of texts to a numerical form\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
        "# Build the LDA model\n",
        "lda_model = models.LdaModel(corpus=corpus, num_topics=6, id2word=dictionary)\n",
        "# Export interactive visuals as html\n",
        "p = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpUlzrBAgUVA"
      },
      "source": [
        "After considering the above visualization, what stands out the most to you? \n",
        "\n",
        "Are these words the most useful to consider for a classification algorithm when learning to distinguish between the three categories we've selected? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxA7KtXGTj_J"
      },
      "source": [
        "#### Normalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i31tNYehI6A"
      },
      "source": [
        "The visualization above has brought to light an important issue: words that are used a lot are words that might not be the most important distinguishing feature when you want to learn a classification model. Also, there are a lot of irrelevant characters such as white spaces, punctuations, etc. that are likely also not relevant, but taking up a lot of 'space'. Therefore, it is common in traditional NLP modeling to take out punctuation, words such as 'the', 'a', or 'it' (commonly referred to as stop words), and to normalize text by taking the dictionary form of a word (lemmatizing).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y8u8t7Kh-qs"
      },
      "source": [
        "text = paragraph_1\n",
        "print(text)\n",
        "print('\\n')\n",
        "print('Text after removal of punctuation:')\n",
        "print('\\n')\n",
        "doc = nlp(text)\n",
        "tokens = [word.text for word in doc if not word.is_punct]\n",
        "text = ' '.join(tokens)\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWKj8rt5i5y5"
      },
      "source": [
        "print(text)\n",
        "print('\\n')\n",
        "print('Text after removal of stop words:')\n",
        "print('\\n')\n",
        "doc = nlp(text)\n",
        "tokens = [word.text for word in doc if not word.is_stop]\n",
        "text = ' '.join(tokens)\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH2rr_uXjI64"
      },
      "source": [
        "print(text)\n",
        "print('\\n')\n",
        "print('Text after lemmatization:')\n",
        "print('\\n')\n",
        "doc = nlp(text)\n",
        "tokens = [word.lemma_ for word in doc]\n",
        "text = ' '.join(tokens)\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YglL5iXskL9j"
      },
      "source": [
        "# putting it all together in some neat functions:\n",
        "def remove_whitespaces_and_newlines(text):\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_digits(text):\n",
        "    text = re.sub('\\d+', '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = str(text)\n",
        "    text = remove_digits(text)\n",
        "    text = remove_whitespaces_and_newlines(text)\n",
        "    doc = nlp(text)\n",
        "    tokens = [word.lemma_ for word in doc if not word.is_stop and not word.is_punct]\n",
        "    text = ' '.join(tokens)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e22Sif7huS7d"
      },
      "source": [
        "# apply preprocess_text to the 20 newsgroup dataset:\n",
        "df['preprocessed_text'] = df['text'].progress_apply(preprocess_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cdNdE_MpMhg"
      },
      "source": [
        "To get an idea of what the functions above have cleaned from the text, consider one of the texts and its preprocessed variant:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymxKzkuZpYda"
      },
      "source": [
        "# to make sure that the whole text is printed in the notebook, you can use:\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "df.text.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iE7Mr0ApKjh"
      },
      "source": [
        "df.preprocessed_text.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPc4TC4djW7n"
      },
      "source": [
        "tokenized_data = df['preprocessed_text'].apply(nltk.word_tokenize)\n",
        "# Build a Dictionary - association word to numeric id\n",
        "dictionary = corpora.Dictionary(tokenized_data)\n",
        "# Transform the collection of texts to a numerical form\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
        "# Build the LDA model\n",
        "lda_model = models.LdaModel(corpus=corpus, num_topics=6, id2word=dictionary)\n",
        "# Export interactive visuals as html\n",
        "p = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbOlA0yjTf6a"
      },
      "source": [
        "#### Vectorize\n",
        "\n",
        "One last step to take before we can feed our data to a machine learning model - we need to make sure that the texts is transformed into digits, as algorithms are unable to compute anything on letters.\n",
        "\n",
        "There are several techniques to transform your text to digits. Here, we will explore TF-IDF or Term Frequency - Inverse Document Frequency and word embeddings. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3N2DYIok38G"
      },
      "source": [
        "*\"TF-IDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general\"* (https://en.wikipedia.org/wiki/Tf%E2%80%93idf). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COKodoskl4h3"
      },
      "source": [
        "We don't need to write out the calculations ourself - `sklearn` makes it easy to transform your text into a tf-idf representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBpn15SxmFbS"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "\n",
        "# create the vectorizer\n",
        "vectorizer=TfidfVectorizer() \n",
        "# fit the tf-idf transformer on the preprocess text of your dataframe\n",
        "X = vectorizer.fit_transform(df['preprocessed_text'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPOW7XRGNEjx"
      },
      "source": [
        "Now let's inspect some transformed texts:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vWTTm0sNJIP"
      },
      "source": [
        "print(df.shape)\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJjbEI8lN1iP"
      },
      "source": [
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGOmFLVjNabS"
      },
      "source": [
        "# create dictionary to find a tfidf word each word\n",
        "word2tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
        "\n",
        "# print first 25 words and their score\n",
        "for word, score in list(word2tfidf.items())[:25]:\n",
        "    print(word, score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOxe5nten88f"
      },
      "source": [
        "# let's look up the texts where some of these words appear:\n",
        "df[df.preprocessed_text.str.contains('aangeboden')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwzPfVqBPYNn"
      },
      "source": [
        "For word embeddings, you can use the `gensim` library. `Gensim` contains pretrained word embeddings that you can use to vectorize your texts. Sind word embeddings are able to encode semantics into the vectorization, they often perform better in machine learning models than tf-idf. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NtKjg-3SBam"
      },
      "source": [
        "import gensim.downloader\n",
        "\n",
        "# download a set of pretrained embeddings\n",
        "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0_etyqPS7HL"
      },
      "source": [
        "# see how powerful word embeddings are:\n",
        "glove_vectors.most_similar('labrador')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaWs8m8bUj7J"
      },
      "source": [
        "# take the text from the first item\n",
        "text = df['preprocessed_text'].iloc[0]\n",
        "\n",
        "# loop over the text to collect word embeddings for all words\n",
        "word_vectors = [glove_vectors[word] for word in nltk.word_tokenize(text) if word in glove_vectors.vocab]\n",
        "\n",
        "# inspect results:\n",
        "word_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGpKADCzp-ZR"
      },
      "source": [
        "Since texts seldom contain the same number of words, you'll have to find a way to make sure that the vectorized texts all have the same number of features, i.e. the same number of words, before you can use them in any machine learning algorithm. There's several ways to do that:\n",
        "- take the means of vectors (MoV);\n",
        "- truncating and padding\n",
        "\n",
        "Here, I'll show you how to take the mean of vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDMBLVd7quSg"
      },
      "source": [
        "np.mean(word_vectors, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhHc2yiRnFeE"
      },
      "source": [
        "## Train & Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AabY6Zr4vAc-"
      },
      "source": [
        "Now it's time to build a classifier that can predict which of the three classes is the best label for any unseen text. First, let's split the data into a train and test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huN_bQPQxN1U"
      },
      "source": [
        "from sklearn import metrics, model_selection, pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGMcXVTYugKL"
      },
      "source": [
        "df_train, df_test, y_train, y_test = model_selection.train_test_split(\n",
        "    df, \n",
        "    df['label'], \n",
        "    test_size=0.2, \n",
        "    random_state=42\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQdiQdRu37s2"
      },
      "source": [
        "#### TF-IDF features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2oV03i6wcqJ"
      },
      "source": [
        "tf_idf = TfidfVectorizer()\n",
        "\n",
        "# pipeline\n",
        "pipe = pipeline.Pipeline([\n",
        "                          (\"vectorizer\", tf_idf),\n",
        "                          (\"classifier\", LogisticRegression())\n",
        "                          ])\n",
        "pipe.fit(\n",
        "    df_train['preprocessed_text'],\n",
        "    y_train\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkNa6fr630DP"
      },
      "source": [
        "Now check the performance on the unseen test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVGQ6GJP331g"
      },
      "source": [
        "X_test = df_test[\"preprocessed_text\"].values\n",
        "predicted = pipe.predict(X_test)\n",
        "print(metrics.classification_report(y_test, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3fj03iHRE9R"
      },
      "source": [
        "## Zero-shot classification with transformers\n",
        "\n",
        "What to do if there's no labeled data? .... zero-shot classification\n",
        "`transformers`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHaK07euRJno"
      },
      "source": [
        "# might take a long time... \n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "\n",
        "text = df.text.iloc[25]\n",
        "print(text)\n",
        "labels = ['science', 'cars', 'graphics']\n",
        "\n",
        "classifier(text, labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}